{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ahmed\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ahmed\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from datasets import Dataset, Features, Value, Image as DatasetImage\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel, TrainingArguments, Trainer\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Step 1: Load and Prepare Data\n",
    "annotations_dir = r\"C:\\Users\\ahmed\\Desktop\\STP\\prescription-data\\annotations\"\n",
    "\n",
    "if not os.path.isdir(annotations_dir):\n",
    "    raise FileNotFoundError(f\"Annotations folder not found: {annotations_dir}\")\n",
    "\n",
    "# Collect image-text pairs\n",
    "image_files = [f for f in os.listdir(annotations_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "data = {\"image\": [], \"text\": []}\n",
    "\n",
    "for img_file in image_files:\n",
    "    img_path = os.path.join(annotations_dir, img_file)\n",
    "    txt_file = img_path.replace(\".png\", \".txt\").replace(\".jpg\", \".txt\").replace(\".jpeg\", \".txt\")\n",
    "    \n",
    "    if not os.path.exists(txt_file):\n",
    "        print(f\"Warning: No text file found for {img_file}\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        # Load image\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        # Read text\n",
    "        with open(txt_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read().strip()\n",
    "        data[\"image\"].append(img)  # Store PIL image directly\n",
    "        data[\"text\"].append(text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {img_file}: {e}\")\n",
    "        continue\n",
    "\n",
    "if not data[\"image\"]:\n",
    "    raise FileNotFoundError(\"No valid image-text pairs found in annotations folder.\")\n",
    "\n",
    "# Define dataset features\n",
    "features = Features({\n",
    "    \"image\": DatasetImage(),\n",
    "    \"text\": Value(\"string\")\n",
    "})\n",
    "\n",
    "dataset = Dataset.from_dict(data, features=features)\n",
    "train_test_split = dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "eval_dataset = train_test_split[\"test\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2443\n",
      "272\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.__len__())\n",
    "print(eval_dataset.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ديكسا ميثازون عند اللزوم\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0][\"text\"])\n",
    "Image._show(train_dataset[0][\"image\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"cross_attention_hidden_size\": 768,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layernorm_embedding\": true,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"trocr\",\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_learned_position_embeddings\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "model.config.decoder_start_token_id = processor.tokenizer.bos_token_id  # Typically 0 or 2\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id  # Typically 1\n",
    "if model.config.decoder_start_token_id is None:\n",
    "    model.config.decoder_start_token_id = processor.tokenizer.cls_token_id  # Fallback\n",
    "if model.config.pad_token_id is None:\n",
    "    model.config.pad_token_id = processor.tokenizer.eos_token_id  # Fallback to EOS if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionEncoderDecoderModel(\n",
       "  (encoder): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTSdpaAttention(\n",
       "            (attention): ViTSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): ViTPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (decoder): TrOCRForCausalLM(\n",
       "    (model): TrOCRDecoderWrapper(\n",
       "      (decoder): TrOCRDecoder(\n",
       "        (embed_tokens): TrOCRScaledWordEmbedding(50265, 1024, padding_idx=1)\n",
       "        (embed_positions): TrOCRLearnedPositionalEmbedding(514, 1024)\n",
       "        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (layers): ModuleList(\n",
       "          (0-11): 12 x TrOCRDecoderLayer(\n",
       "            (self_attn): TrOCRAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_fn): GELUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): TrOCRAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (output_projection): Linear(in_features=1024, out_features=50265, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_example(examples):\n",
    "    # Images are already PIL objects from the dataset\n",
    "    images = [img.convert(\"RGB\") for img in examples[\"image\"]]  # Ensure RGB format\n",
    "    pixel_values = processor(images, return_tensors=\"pt\").pixel_values.to(device)\n",
    "    labels = processor.tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).input_ids.to(device)\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Par', 'acet', 'am', 'ol', 'Ġ500', 'mg', 'ĠÙ', 'Ĥ', 'Ø±', 'Ø', 'µ', 'ĠÙĪ', 'Ø§Ø', 'Ń', 'Ø¯', 'ĠÙ', 'Ĭ', 'ÙĪ', 'Ùħ', 'ÙĬ', 'Ø§', 'Ù', 'ĭ']\n"
     ]
    }
   ],
   "source": [
    "print(processor.tokenizer.tokenize(\"Paracetamol 500mg قرص واحد يومياً\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2443/2443 [00:41<00:00, 58.78 examples/s]\n",
      "Map: 100%|██████████| 272/272 [00:03<00:00, 75.81 examples/s]\n"
     ]
    }
   ],
   "source": [
    "processed_train_dataset = train_dataset.map(\n",
    "    process_example,\n",
    "    batched=True,\n",
    "    remove_columns=[\"image\", \"text\"]\n",
    ")\n",
    "processed_eval_dataset = eval_dataset.map(\n",
    "    process_example,\n",
    "    batched=True,\n",
    "    remove_columns=[\"image\", \"text\"]\n",
    ")\n",
    "\n",
    "# Evaluation Metrics (CER)\n",
    "cer_metric = evaluate.load(\"cer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_str = processor.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=labels_str)\n",
    "    return {\"cer\": cer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrOCRTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        outputs = model(\n",
    "            pixel_values=inputs[\"pixel_values\"],\n",
    "            labels=inputs[\"labels\"]\n",
    "        )\n",
    "        return (outputs.loss, outputs) if return_outputs else outputs.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./trocr_finetuned_khatt_medical\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    fp16=True,\n",
    "    dataloader_num_workers=0,\n",
    "    metric_for_best_model=\"cer\",\n",
    "    greater_is_better=False,\n",
    ")\n",
    "\n",
    "# Step 7: Initialize and Train Model\n",
    "trainer = TrOCRTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=processed_train_dataset,\n",
    "    eval_dataset=processed_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='918' max='918' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [918/918 5:56:20, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.079100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.064500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.061000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.051100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.059900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.054300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.046100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.044800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.066700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.073400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.054200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.044000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.051900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.050500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.039200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.056600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.054300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.052800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.038600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.051100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.037800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.047200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.033700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.039500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.041000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.028900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.032900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.041300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.029400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.028700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.033500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.025900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.019300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.014700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.022200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.025600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.015600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.021700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.024700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.018700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.017400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.016500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.016000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.014200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.011200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.015300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.022500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.019300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.018800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.018400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.022800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.024600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.020800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.020800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.015100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.018700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.019500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.016300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.018000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.022600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.021400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.013300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.011300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.011300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.011400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.010700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.006300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.012000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.007200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.010400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.012800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.007300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.012500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.008400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.010200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.008100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>0.007800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.009400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>0.006000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.008100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>0.008000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.007800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>0.005700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.008300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.006000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.003900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>0.007900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.006200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>0.003900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.004700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>0.005100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=918, training_loss=0.02529888218654267, metrics={'train_runtime': 21394.5891, 'train_samples_per_second': 0.343, 'train_steps_per_second': 0.043, 'total_flos': 5.484182164809449e+18, 'train_loss': 0.02529888218654267, 'epoch': 3.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./trocr_finetuned_medical\")\n",
    "processor.save_pretrained(\"./trocr_finetuned_medical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainer\u001b[38;5;241m.\u001b[39mpush_to_hub(\n\u001b[0;32m      2\u001b[0m     token\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf_xKCMLLZhcoctfDJlptmujWaxMPXnearxVf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "trainer.push_to_hub(\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"cross_attention_hidden_size\": 768,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layernorm_embedding\": true,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"trocr\",\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_learned_position_embeddings\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processor = TrOCRProcessor.from_pretrained(\"./trocr_finetuned_medical\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"./trocr_finetuned_medical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ahmed\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ahmed\\.cache\\huggingface\\hub\\models--wahdan2003--trocr-handwritten-medical. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "model.safetensors: 100%|██████████| 1.34G/1.34G [1:07:45<00:00, 329kB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/wahdan2003/trocr-handwritten-medical/commit/331492386a896c8284cee08de9d4a54ebb5605f9', commit_message='Upload processor', commit_description='', oid='331492386a896c8284cee08de9d4a54ebb5605f9', pr_url=None, repo_url=RepoUrl('https://huggingface.co/wahdan2003/trocr-handwritten-medical', endpoint='https://huggingface.co', repo_type='model', repo_id='wahdan2003/trocr-handwritten-medical'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\n",
    "    repo_id=\"wahdan2003/trocr-handwritten-medical\"\n",
    ")\n",
    "\n",
    "processor.push_to_hub(\n",
    "    repo_id=\"wahdan2003/trocr-handwritten-medical\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_path = os.path.join(annotations_dir, image_files[0])  # First image as example\n",
    "new_image = Image.open(inference_path).convert(\"RGB\")\n",
    "pixel_values = processor(new_image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "generated_ids = model.generate(pixel_values)\n",
    "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(f\"Predicted text: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"cross_attention_hidden_size\": 768,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layernorm_embedding\": true,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"trocr\",\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_learned_position_embeddings\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VisionEncoderDecoderModel(\n",
       "  (encoder): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTSdpaAttention(\n",
       "            (attention): ViTSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): ViTPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (decoder): TrOCRForCausalLM(\n",
       "    (model): TrOCRDecoderWrapper(\n",
       "      (decoder): TrOCRDecoder(\n",
       "        (embed_tokens): TrOCRScaledWordEmbedding(50265, 1024, padding_idx=1)\n",
       "        (embed_positions): TrOCRLearnedPositionalEmbedding(514, 1024)\n",
       "        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (layers): ModuleList(\n",
       "          (0-11): 12 x TrOCRDecoderLayer(\n",
       "            (self_attn): TrOCRAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_fn): GELUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): TrOCRAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (output_projection): Linear(in_features=1024, out_features=50265, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Paths\n",
    "\n",
    "\n",
    "# Load TrOCR processor and model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAoAAAADJCAYAAACzBYOuAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAANMtJREFUeJztnXl0VdXZ/5875Ga4CUkISZgTIIAgYWiQVpShiqAMS7FK1apAFy1LW8W62v66bB2wLnj7s4NWfdX+fF/0Vd8llTq0VrRQpE4UBwYFRQYDGqAQAoQh4733/P5wkeW9zzfweM9JsJ7vZy3XMg/nnL3PPnvvs3Py/e4n4DiOI4QQQgghxDcET3cFCCGEEEJI58IFICGEEEKIz+ACkBBCCCHEZ3ABSAghhBDiM7gAJIQQQgjxGVwAEkIIIYT4DC4ACSGEEEJ8BheAhBBCCCE+gwtAQgghhBCfwQUgIV9RysvLZc6cOW0/r169WgKBgKxevfq01SmV1DqSZB599FEJBALyzjvvdHhZd9xxhwQCgQ4vhxDy5YALQEI6gBMv7hP/ZWVlyaBBg+SHP/yh7Nu373RX7wvx4osvyh133HHaym9oaJA77rijUxau//u//yv33HNPh5fzVefEYvLAgQOnuyqEkHbgApCQDuTOO++Uxx9/XO6//34ZO3asPPjgg3L22WdLQ0NDp9dl/Pjx0tjYKOPHj/9C57344ouycOHCDqrVqWloaJCFCxdyAdjB/OIXv5DGxsbTXQ1CSCcRPt0VIOSrzEUXXSSjR48WEZF58+ZJUVGR/Pa3v5Xnn39errzySnjO8ePHJRqNel6XYDAoWVlZnl+XfDUIh8MSDnfuK6GhoUFycnI6tUxCyGfwCyAhnch5550nIiLV1dUiIjJnzhzJzc2VHTt2yNSpUyUvL0++853viIhIIpGQe+65R84880zJysqS0tJSmT9/vhw6dCjpmo7jyF133SW9e/eWnJwc+eY3vymbN29WZbenAVy7dq1MnTpVCgsLJRqNyvDhw+Xee+9tq98DDzwgIpL0J+0TeF3HVHbu3CnFxcUiIrJw4cK28j//J+ktW7bIZZddJl27dpWsrCwZPXq0/PnPf2779/3790txcbFMnDhRHMdpi2/fvl2i0ah8+9vfFhGRiRMnyl//+lfZtWtXWznl5eVtx3/yySeyZcuWU9a5paVFbrvtNqmqqpL8/HyJRqMybtw4eeWVV9SxTz31lFRVVUleXp506dJFKisr29r+8zQ3N8vNN98sxcXFEo1GZebMmVJbW5t0THl5uUyfPl1Wr14to0ePluzsbKmsrGx73s8884xUVlZKVlaWVFVVyfr165POb08D+MQTT8iYMWMkJydHCgsLZfz48fK3v/3tlO2QysSJE2XYsGHy7rvvyvjx4yUnJ0duueWWL3wdQog38AsgIZ3Ijh07RESkqKioLRaLxWTKlCly7rnnyq9//eu2LyLz58+XRx99VObOnSs33nijVFdXy/333y/r16+XN954QzIyMkRE5LbbbpO77rpLpk6dKlOnTpV169bJ5MmTpaWl5ZT1WbFihUyfPl169OghCxYskO7du8uHH34oL7zwgixYsEDmz58ve/bskRUrVsjjjz+uzu/oOhYXF8uDDz4o1113ncycOVMuvfRSEREZPny4iIhs3rxZzjnnHOnVq5f87Gc/k2g0Kn/84x/lkksukT/96U8yc+ZMKSkpkQcffFAuv/xyue++++TGG2+URCIhc+bMkby8PPnP//xPERH5+c9/LvX19VJTUyO/+93vREQkNze3rS7XXnut/OMf/0haRCKOHDkijzzyiFx55ZXyve99T44ePSr/9V//JVOmTJG33npLRo4c2db2V155pZx//vnyq1/9SkREPvzwQ3njjTdkwYIFSde84YYbpLCwUG6//XbZuXOn3HPPPfLDH/5Qli5dmnTc9u3b5aqrrpL58+fL1VdfLb/+9a9lxowZ8tBDD8ktt9wi119/vYiILF68WGbNmiUfffSRBIPtfwdYuHCh3HHHHTJ27Fi58847JRKJyNq1a2XVqlUyefLkk7YDoq6uTi666CK54oor5Oqrr5bS0tIvfA1CiEc4hBDPWbJkiSMizsqVK53a2lrn008/dZ566imnqKjIyc7OdmpqahzHcZzZs2c7IuL87Gc/Szr/tddec0TEefLJJ5PiL730UlJ8//79TiQScaZNm+YkEom242655RZHRJzZs2e3xV555RVHRJxXXnnFcRzHicViTr9+/ZyysjLn0KFDSeV8/lo/+MEPHDRVdEQdEbW1tY6IOLfffrv6t/PPP9+prKx0mpqakuo+duxYZ+DAgUnHXnnllU5OTo6zdetW5+6773ZExHnuueeSjpk2bZpTVlYG6zFhwgTYDqnEYjGnubk5KXbo0CGntLTU+e53v9sWW7BggdOlSxcnFou1e60T/WjSpElJbfejH/3ICYVCzuHDh9tiZWVljog4b775Zlvs5ZdfdkTEyc7Odnbt2tUWf/jhh5P6guM4zu233550f9u2bXOCwaAzc+ZMJx6PJ9Xr83VBnLhWbW1tW+xE+z300EMnPZcQ0jnwT8CEdCCTJk2S4uJi6dOnj1xxxRWSm5srzz77rPTq1SvpuOuuuy7p56efflry8/PlggsukAMHDrT9V1VVJbm5uW1/Tly5cqW0tLTIDTfckPTnu5tuuumUdVu/fr1UV1fLTTfdJAUFBUn/ZtkOpDPqeDIOHjwoq1atklmzZsnRo0fbyq+rq5MpU6bItm3bZPfu3W3H33///ZKfny+XXXaZ3HrrrXLNNdfIxRdfbC5v9erVp/z6JyISCoUkEomIyGd/Ij948KDEYjEZPXq0rFu3ru24goICOX78uKxYseKU1/z+97+f1Hbjxo2TeDwuu3btSjpu6NChcvbZZ7f9/PWvf11EPpMe9O3bV8U//vjjdst87rnnJJFIyG233aa+Eqa7XUxmZqbMnTs3rXMJId7CPwET0oE88MADMmjQIAmHw1JaWiqDBw9WL9NwOCy9e/dOim3btk3q6+ulpKQEXnf//v0iIm0LgIEDByb9e3FxsRQWFp60bif+HD1s2DD7DXVyHU/G9u3bxXEcufXWW+XWW29ttw4nFttdu3aV3//+93L55ZdLaWmp/P73v0+77FPx2GOPyW9+8xvZsmWLtLa2tsX79evX9v/XX3+9/PGPf5SLLrpIevXqJZMnT5ZZs2bJhRdeqK73+cWbiLS1W6rWMvW4/Px8ERHp06cPjKee/3l27NghwWBQhg4d2u4xX5RevXq1LY4JIacXLgAJ6UDGjBnT5gJuj8zMTLUoTCQSUlJSIk8++SQ854Qx4nRyuuuYSCREROTHP/6xTJkyBR5TUVGR9PPLL78sIp8tfGpqatSXTy944oknZM6cOXLJJZfIT37yEykpKZFQKCSLFy9uW3SLiJSUlMiGDRvk5ZdfluXLl8vy5ctlyZIlcu2118pjjz2WdM1QKATLSv0i2d5x1vM7muzs7E4tjxDSPlwAEvIlZMCAAbJy5Uo555xzTvrSLCsrE5HPvsb179+/LV5bW3vSrzsnyhAR2bRpk0yaNKnd49r7c19n1PFk5Z+4VkZGxknrf4KXXnpJHnnkEfnpT38qTz75pMyePVvWrl2btPWJF5kwli1bJv3795dnnnkm6Xq33367OjYSiciMGTNkxowZkkgk5Prrr5eHH35Ybr31VrV47WwGDBggiURCPvjggzbjCiHkqwM1gIR8CZk1a5bE43H55S9/qf4tFovJ4cOHReQzjWFGRobcd999SV9zLJsZf+1rX5N+/frJPffc03a9E3z+Wif2JEw9pjPqKCJtrujU8ktKSmTixIny8MMPy969e9V5n98m5fDhwzJv3jwZM2aMLFq0SB555BFZt26dLFq0KOmcaDQq9fX1sB7WbWBOfG37/L2uXbtW1qxZk3RcXV1d0s/BYLDN3dzc3HzKcjqaSy65RILBoNx5551tX1tP0BlfDuvr62XLli3tPg9CiDv4BZCQLyETJkyQ+fPny+LFi2XDhg0yefJkycjIkG3btsnTTz8t9957r1x22WVSXFwsP/7xj2Xx4sUyffp0mTp1qqxfv16WL18u3bp1O2kZwWBQHnzwQZkxY4aMHDlS5s6dKz169JAtW7bI5s2b2/5cWlVVJSIiN954o0yZMkVCoZBcccUVnVJHkc/+bDh06FBZunSpDBo0SLp27SrDhg2TYcOGyQMPPCDnnnuuVFZWyve+9z3p37+/7Nu3T9asWSM1NTWyceNGERFZsGCB1NXVycqVKyUUCsmFF14o8+bNk7vuuksuvvhiGTFiRNu9Ll26VG6++WY566yzJDc3V2bMmCEi9m1gpk+fLs8884zMnDlTpk2bJtXV1fLQQw/J0KFD5dixY23HzZs3Tw4ePCjnnXee9O7dW3bt2iX33XefjBw5UoYMGXLKduloKioq5Oc//7n88pe/lHHjxsmll14qmZmZ8vbbb0vPnj1l8eLFHVr+s88+K3PnzpUlS5YwXzQhHcHpsh8T8lXmxPYdb7/99kmPmz17thONRtv99z/84Q9OVVWVk52d7eTl5TmVlZXOT3/6U2fPnj1tx8TjcWfhwoVOjx49nOzsbGfixInOpk2bnLKyspNuA3OC119/3bngggucvLw8JxqNOsOHD3fuu+++tn+PxWLODTfc4BQXFzuBQEBtheJlHdvjzTffdKqqqpxIJKK2hNmxY4dz7bXXOt27d3cyMjKcXr16OdOnT3eWLVvmOI7jPP/8846IOL/5zW+SrnnkyBGnrKzMGTFihNPS0uI4juMcO3bMueqqq5yCggJHRJK2hLFuA5NIJJxFixY5ZWVlTmZmpjNq1CjnhRdecGbPnp10vWXLljmTJ092SkpKnEgk4vTt29eZP3++s3fv3rZj2utH6FmWlZU506ZNU/UREecHP/hBUqy6utoREefuu+9ui6VuA3OC//7v/3ZGjRrlZGZmOoWFhc6ECROcFStWnLQNbrvtNkdEnIMHD7bFJkyY4Jx55pknPe/znLj3JUuWmM8hhNgJOE4nq4AJIYR8pbn55pvl3nvvlaamprbNwAkhXy6oASSEEOIpb7/9tlRUVHDxR8iXGC4ACSGEeMKSJUvkmmuukddff11mz559uqtDCDkJ/BMwIYQQTwgGg9K9e3e55pprZNGiRe3uP0gIOf1wAUgIIYQQ4jP4J2BCCCGEEJ/BBSAhhBBCiM/gApAQQgghxGdwAUgIIYQQ4jO4ACSEEEII8RlcABJCCCGE+AwuAAkhhBBCfAYXgIQQQgghPiNsPfCF55aBqN5D2knoWCAQ0MeB/afRcSjmBmu5lvOsx1ljaNd8a7nWNvaaRCKR9rm42VFQ30cC9jNwJmyD9PqAtX9anzdqO3S9YFD/nmY9102/tY9HdJztd0s39bP2PUu7wPsKpj9vWZ9PANy+1+MW9R83z9tN/by+Nzf9x4K17axldsY8be17CDfvG+v13Jzb0e9vN/O5m7WK133g21fPMR3HL4CEEEIIIT6DC0BCCCGEEJ/BBSAhhBBCiM/gApAQQgghxGeYTSBIm490i1YhpBtxrRshrdemknTrgXBnqEjfPICwnouMKwh37ZL2qe0YTU4Nun+Em/uyjoGOFrp/kXLt7ZJ+P7PixlxmARra4IGgnWDd0qpGu3htRrCe67UAHuG1WSL1eqfLoOEGaxu7MVN2hukF4caA5Mb0km6ZXyZzlJuxxy+AhBBCCCE+gwtAQgghhBCfwQUgIYQQQojP4AKQEEIIIcRnmE0gASB/dkBGBbR7vlX9DIXTpjO9zyxiEWW6MZ64MRl0xi7siM4w3+A6m06FhELpGS3cGGgQ1mfhxhTRGRkFENasH27o6OwY8F5RPwHXQnMjHiu6DHg9j8eZG4PY6RK2e20+6Wgzh9dzsps+gJ6tmwwXVrzOlGTFTQYty3le9x2vM4a4MiKmfSYhhBBCCPm3hAtAQgghhBCfwQUgIYQQQojP4AKQEEIIIcRn2E0gwNwRDujTkSAxHo+bykDnWjNNeC2sTL2edYd0r40CnWEK8Fp07kbA7fWO9fh5nLoubgTxXpt+rGV4fT17nXUsCOYLrzMFuMFTYbexamguQ3Ojuwws6WctsI5HN9mE3IjYvezzbuY8r+dpaxleX8/anm4ybbgxrrh5v6Ye5/W7BuH1e68zMpDwCyAhhBBCiM/gApAQQgghxGdwAUgIIYQQ4jO4ACSEEEII8RlmEwgCCZiRSDEc1sUgMWc8BswiQPMZCmoxdUeL4q3XR0Jvq6HgdAm9EW5E914bYRD4fm3HeVkPK16blLwu141IOpHwVsRvPddqUEi3/dy0ideibjfmCdQm1kwgpyvThtdZRCz3YX0W1kwbbspAeG0o6IysFwivzTaW/m01lHj9fvTatMFMIIQQQgghxBVcABJCCCGE+AwuAAkhhBBCfAYXgIQQQgghPsNsAvFSXN3euQEkrjWKPhFe70SfSmcYL9xcz5WpBKQ3cEAGDTfl2rMMoHKRQNiNaSE90w/m9BhI3Aj23e2UbzPkILw2HlhF4pYyrXOFdZ5BeG3GsJ7rtfHAitfGHWsGm9QyrM/MjeHDzT24ycBixesMLFY6w2ihj0HvpPTHipsxinBl5HCRhYZfAAkhhBBCfAYXgIQQQgghPoMLQEIIIYQQn8EFICGEEEKIz3CVCcSNycJrITHCa8F2Kl6LchFe7ySO/RQ6GAMC2SASXENxbfpgQSsSiaO2R/0CxXQZqUYGe5+1GVSsj8xrwbW1DDemAByzlmsq1lW7pDvm3WRZsF4Pn+tt//G6zgg3/cdNuV4b59IF3heYG83vDKMxLxDwNjMEbifUxmhO1me6MSJasfQBPAekPy6s95VIgOxmgM6Y9xH8AkgIIYQQ4jO4ACSEEEII8RlcABJCCCGE+AwuAAkhhBBCfIbZBOJGlGsVTMbjWjBpFvQaRecIi0jculu9l2V+kTLMdYHVA6YIKIa1XQ+dmgDPNoT6Bby39O8X90ddrOU81AdQFgyciUCXgY0s6DhvhcnWMqy73Vv1y7A7AqG8GyODta1S7w1dPxQKma7lypQFJi6rEB9ltDld2Yms53YGlv7jZv6AYwW1CTRFoFeuzQgEnzc409qn7OYo25yMH7fVIAbORE3qcRajdK/lZu5BVbOOZa9NofwCSAghhBDiM7gAJIQQQgjxGVwAEkIIIYT4DC4ACSGEEEJ8hisTCBbFp7+mNItSgWASYc1GYDkXizRtZUJZLRSzmqrmSvQJtLtQiB+M227EAVk6EsDcEAgZDSlm4W/6/cwirsUGCFSmTaxtrYc904YtZjUyIOxtnL4xxI3JyWpSQaS2C7o+MqUhOid7i46hZ4vaxG7m6fisGl+WTFHpvgdERBxjdqHWVt1/gsGYikUybK9h4DdrZ4zaDItuxh7qe7gM1PdUyGw+QcCjlOnHmrXLlq3odI0pr8cKvwASQgghhPgMLgAJIYQQQnwGF4CEEEIIIT6DC0BCCCGEEJ9hNoFYsWbMQLgRTAZBLI7SLwDQruYqUwAyqACxP9L/BwI6GAxqES32RNhEtIgAyh5gFsOC6wEVMmo7+HsFzOYBynCxm7xVNIxtOafGa2MDLsOWkcIq6kZGBnu2HmQgSd+Qgknf0NTh5gtbwiHcL8zCcVtVcHPaxrKb7AGny7Th9fXSNc4lEnr8hMMZKnag7riK7dn7LxXr3acUXA+Y8OAYtc2hVlyZCdF73piBBJWK7heaTG1JctR7zrwGQfMb/E5mdICCw+wZWFARbrIOgbp4ejVCCCGEEPKlhwtAQgghhBCfwQUgIYQQQojP4AKQEEIIIcRnmE0g1qwf1p3oEWbhLzgsjsTKUCRtMyhIILlpEglkPNHXCgPdfCymd3+vO3hExeJg5/j8/DxdRgYSCOtyE1iyDmIAo/g7IyNTxfYfOKpiW7fuUrGRI8pVLDs7omLAQwPNJ26MFhaw+N12fWsmHWu5rgTc5swiSOgN6mzM3oKwCqKtGYbwfdiOU8cYU/0kUL8A14P3Cgs+PYJwr/tZ52RIsY2r9O9DXz8jQ89RH27ZrmJr39Gxb1/WVcVys6MqFoy4ebZg/kFHoVeh0VwXN5oJ4TgAZSDDpoPMkwH9go2jrFUp81QopI07ra36vYzmPGTSQTeGjKLW96h1zvPS4CTCL4CEEEIIIb6DC0BCCCGEEJ/BBSAhhBBCiM/gApAQQgghxGe4ygTiJsOHVSBsFjjCrBI2w0drixaDpgpLszK1UPfYcb37e+2BOhXbsmWHPq52v4rl5eWoWHlZbxUbMmSAimVlapGrFdzEQOwe1wLZ7Gwtyv3zSx+p2EurPlSxu+/U91aeo00lMZBeBT1b2Feg+vnU4n43gnh8+fSv57XYH7ed9XdBtLN/+mYja1YJd6YXJFgPpBwDjjBmMcAn28wo5jKMWM0ybox5bvqt19fD2VVO/b5xUzfUxq0JbQzZ/mmritUd1O+MrgXZugxgWgjBR2tMjRGwGWOQ2RF5JMMhfb+xmL5fNK/EQXYVcfR7pKW1RcWaW3QZDY061pryrmpo0GU2HNdGzO7d9Hu+W0mBimVG9NKpM4x5CDdmK34BJIQQQgjxGVwAEkIIIYT4DC4ACSGEEEJ8BheAhBBCCCE+w2wCsWb98BqcKcB4MjguHNbi2j27D6jYm2vWJf2cnZ2rjmlsbFSx5mYtXM3K0oLZCRPOVbH8fC1AbW3RZYRBuhFoeEGANkkgkbyjRbORiDZo7NixW8UOHdd1DkW6qdiuXTpjyIC+hSoWd5pVDCRDgeJ5uEd8XN9bIFU4bRSJJ4CgORi0mZ6sWQzcZDixit2huQMONJRlAInOrSL79LOhWEEi9tRisTEGXQ21HRBwo7nR2CZuwP0HHgliqI+C1EbgXGvfc2O+cNMHUtvFjRklBjJIdMnVr9LWVl3GtmptEqwYoOdGNC7AtAX7Lbq3YACYFvTl5Ch4fx050gRieo5vbdEVPNagz609qOfzo8d0ufWH9Ln1hxpUrAmYRVJfQYcPa/NNz266vnOuGatiYeC+MXtTXRiXrOsrq/ELnpv2mYQQQggh5N8SLgAJIYQQQnwGF4CEEEIIIT6DC0BCCCGEEJ9hNoEgkWIoqAXCCReiR6SsjKNzcQVVKAiFlbqM4pIiFRs8eGDSz0eOatHr4MGDVKxbcYGKFRRqA0k4qO8r4egdzUMhfW4rECGjdhe0q7s+CgKF/SDW0KjFtXOv0ELa+rq1Krbr04OgiL6gVCDCBXcSgGJdsJO/QXifiNtE6KgeqI+FQsi4k36GHCt2Y4ibrCQgZswuY8VV5gYglE8Y7hdn7rDeq80Y48b0YwUZCnCGC5vBxY1g3c29ucp4YBDKW+8hntDzb58e2rxWmKdNc9W79JzX2KTn/UiGfjUHg2j+QWYjfW49eH+teWuvir29UcfiMW2yyMwE76+4Pu7AAZ1t43gjGhs6G0pWhp4zuxfp44oLdawwL7mtvjasRB1z1midiapvX23ICQaBaRDOH95mMPI6IxKCXwAJIYQQQnwGF4CEEEIIIT6DC0BCCCGEEJ/BBSAhhBBCiM8wm0AQWAaJzBhgt3K09kQ75QvY/hyVjATCUESprwf0+TJiREXSz+EMnUEE6W9RBoCWGMgY0grE30CEHY9pwXEACpqRQNhqMrD9HhCP67qU9eulYnldclSstIeOffzpYRVrjYHng7KcgL4CBbLGbBap56JngYw2IZApAe/ij4TEYKyAcu27zusYFLajoQKmgsOHdKaWrCw9DiKZYGy0MzukEoL3m74hBXrLwEB1JPm5ofOQWagBCPaD4F4zM3X2H3uumvTBwnFbP7NmQ8FTrZssNBqcASr97CCp5boR08diug90LdBGhJLiLBXbvXufih05oo10XaLaQCJorBgNZ6+u2axif/3bHhUr7a6NEaNH6VhZnwIVa2zQ8wWqnwAzSzRbj5duBV1ULCsTGU/BOyOc3H+iObo9w2D1EweZnVC2J5w1B40z3Y+tfZsmEEIIIYQQ4jlcABJCCCGE+AwuAAkhhBBCfAYXgIQQQgghPsNsAkEidihchAJ7IP4OGXedB9dDmRYEGDni4LhwUN9yOKxPbm5Jvt/GZr3L+eH6YypWXX1IxWKtTSo25qx+KpYJxKGJoBYcQx1oQt8D8orAVkfZW1CGC2NGgVirNr0MGaSzrSz7oEbFjjc0q1g2EP4iMw8yPAShSePUQlookgf9WDzOgICrZsvKYjUUhMNacF1X16Bijz7+dxWbOHGkig0/U2dvCaLxDQ0a4DBjW9mzCenDMsLJ59Yf0uPs1TUbVKy+Xo/vPr11loHyvt1VrHv3rioWNmavsQrM3YjEkXkJfyNAAnh0vfQzn7gxkKSbMQTWA9QXTQNh0N/79ChQsW07tPHiGJjzkOEMPluj6ae0m85UctYo/U7LztTndsnV5r+uhXoOyempx0FmRPef7Cz9DkavdJRxBfhHRBxLlhfdxrFW0I+N70w3fdbrDFBuxjy/ABJCCCGE+AwuAAkhhBBCfAYXgIQQQgghPsPVRtAQvBMrOA5oK5AQAPwFPhzSG882tepzm5u1huDoUa3b+9derevZd6A+6efaOr3JZfWu/SrWIsUqtvcTff2+5Voj1L9Pnr5ei26nSEjrL5pj+rhDB+tVLJKpH3lel6iK4ccINA5oc9K4jg0s76ZiiZi+3p69+vkMqtD6lZYWrQFEmigsizu1BsOuq0DXsuoOkc7HW61XMKjrEgLXa2nV7fn+h/pZ7K19V8X69tH6zqKuuaA2SOOL2kCfCTU3cPNzTWZEzxfvf3Ag6eelS/+pjmlu0uNn5KgyFduwfouKHTtyWMUGlJ+vYk3NWi8LB58R3C1sbdzOFcG53urzTscmuOZ7QBvCo03dw/p6PXuAMQAEqbUH9EbQFf31qUighqoci2tt34jKchUr66PfQRs2blexLR9sU7GaT3erWEWFTgrQA+gg86J60+xssMF8AG3gbhwaqc8yCLS25j5m1Lda6vFF8HrTZwS/ABJCCCGE+AwuAAkhhBBCfAYXgIQQQgghPoMLQEIIIYQQn2E2gbgRoiOxNtpYOhQAG9Qe1gLZ6l2HVWzLtoMqtmdPnYol4nrD22BAx3Lzkg0ZxcXaiDAJbIpb3FuLxH9372oVO3xUbzwbDIHNnEGbSEA/tmV/eUvFtm/bqWI9SvS5E8aNVrGBQNALt4kF4tpYTIuQS7vlqFhmhj73kxotih86WBtIWkUbfBCo78HjUvqyWbwL9zu2nYs2NLeL323C/hB4PmjsdS3IV7G+vfXGxRs37VKx3cC4U1SkDU1IwY029IbtB9olkdDnZmZmqdjW7drA9R+/Td7keuxZFeqYb3/rQhXLy9f9eMXK1So2qKJcxdA9oHk1gMxMANQv0AbmDtwoN71x0V4MDQRcF9TnYcmmMrw0i1jvC8UcUI+SEm0CyQLTeXW1NlR8DZiNMoD5D21cLIIMcvqooq56rEwcN0zFDpyh6/LOuq0qtmG9NpDUH+2pYkMH6VhGGJgdI5kqJmCzcstcjeZa4zCD75AgNOR4a3Ayr6+4ETQhhBBCCLHCBSAhhBBCiM/gApAQQgghxGdwAUgIIYQQ4jPMJhAkHEdgMaMWPcZA5oG8XCCw/psWmy798w4V6zdAmxaqRvZVsf79tJmjT08tgC/skrwzeTSq18qhkL6vVtCkFQO6qFg0R98rauLsLC3U3fjeXhVbsbJaxW66cYqKHa77VMU+2qrFu0OA8Le5WZs7AkFd6TjQpAJtvpSVg6wp+3T2hUCgXJ+MhNjAphLAKml9rpP6szFTABT0moqEBgAkTE4g9w0Up+sDE+BkJIjOjeoHdEaFNoG8936Nir27XhtDKgboc7My9W7/8OlAY43twKCjjVRLn92oYuVlvZN+vm7eWHVMwtH9PQ5MZOeePULFoiDbQXNLs4pZzUYOEL9b+yguIn1DhTXbCMaaQcF2NTS+YRYjQ7FWIT5uJz0P9uyhx0D3Ev2uqanRc15LozYJxsK6b2cAIx3K9IPuA5moQmF9b717FahYYeFZKvb+Zj03/PPd91WsIFebO7rk6vc3zv5jM+Xoa6H712MqBIyYVlMfLrdjM9q4hV8ACSGEEEJ8BheAhBBCCCE+gwtAQgghhBCfwQUgIYQQQojPMJtArOBd/HUI7RLfAkwGRV21cHziOb1VLK9Q75KeFTykYnt37lOxA7t1XTIzk8WgXfK1qDsvV+/0HsrUx0W130PCYOdzJErFbadFpGW9tHgVeGqkov9wFas/eEDFWluR4QMJWpE4XZebkaHP7dNHZ4v4YLN+Pq3AMGQFyXfhfv/qQJvw19Xu79C0octImEXDNsE+EsnH4tqgMGH8UBWrO6SfxYZ1O1Vs0oQBKtazd5EuGNQZtYEDsxvoPn+8QWeIqd23X9dvYvI4CIru701gDISDunJ5UT33xB2UqcYmYEfzgNF/JLgPWDOGuMlGYKsLxs1YQw6pU2f0MGf6gfVAJgtdj5xs3T/POEObHVa/9p6Kfbpbz8lDztDvvUBAl4GyZTioTxlnx1aQ2SmSpdugqmqgir351scqVr1L39sZQ/oYaiIC+4ohgYs1k5mbMeAmWxoCj1tvv9nxCyAhhBBCiM/gApAQQgghxGdwAUgIIYQQ4jO4ACSEEEII8RlmE4hZfAiyfkAhpNFQMGHCGSo2boI+90h9o4odO6pjDY3HVezo0aMqFk/ZJR2ZIg4f1iaT7BwtVm8+fkTFGo9rkXg4oh/H8WP6HioGdVexKZN0O73y99dVrFv3UhUbCLKo5OdrU0BOtt7BHRkZoBgW+Dj6l2lTwGv/0KLhQ4d0G+Tn67aKGw0UaLf3eCy5gk4Apt9QBByQzQNlY0Ang7ESAMYGpHNH/RHt4o/qgsqNxXR/7AMyAMy6bLSKLbp7t4q9u1FnB+nVt5uuSlzfbxDt2o+yfoA5BMV6d9dZTnbXfJL089FGLUIPAaMWIgZNGyhDBXi24OFas1sgsOgcZOtBgwVgNUu4yXhgvTdsjjFmhkjN9AOdA8C8BTwWAjLpxGK2rBKDB5Wo2Otr9PPZ+pEeU/3791SxDNBH0fixZzQBYwq0Z2ZEG582ba1Tsfc/qlWxM4cOUbFoNsqMpbOhIBxwv3DeSz3GuKaxG6Fs2UasdUHPx3o9K/wCSAghhBDiM7gAJIQQQgjxGVwAEkIIIYT4DC4ACSGEEEJ8htkEAsW2UBuJMh4AkWYICOCRbhOIHsNg3VpSrLNylJboTBNImAtl0yki6QASpse1cD6ao8tsON6kYq+tXg3qdpaKFRV2UbFsrWmXc84ZpWIjRlaq2KpXt6vY/733NRW78/9coGIDKrRQtwlkb0Gi4eZmLXQe1E+bQDIy9fPZXn1Qxb4xuoeKtTaBbBFAUL9tx14V69o1P/nnQn2vrTEtSkb9ohm0SSSiM9oEgYEmI6Qf7po1m1SsoUH3qbO/MUiXm6XF2kgkjsY3Mpr07pWvYiNGlqnYug06o8s3x+s6R3NA/cCshH5TTQAjQ1ZEHznx3MEq9pe/vpr08z/XFqhjKodXqFhejq5cKGw0fKC7QOJ8YGRB/cxuskg/W40V67luMhngrEM2I0PqLB8EnSwe123S0qT7bEZYP1v4egT17dMbmPAqtAnvo+07VayqVmfXyexVrGOgfsGgnn8yIuA+wNx97JieB9a88amK/b/H/qFi/fsVqtj4c89UsXgCGT6MfdSQMcPrseImO4jnuBi3/AJICCGEEOIzuAAkhBBCCPEZXAASQgghhPgMLgAJIYQQQnyG2QQCgb4QsFu78VxB58KsCsCQ4WihKto1uzUGxJsgFAeZKxRArB2rP6xiZ48ZqusGdjn/x+q3VAztRJ/fRRtDirrqLAu5edqQsm2bNkCUFGnjQWGhNtXEWm1th7JoxOI6s0hRgb6PfmW63D3/0plagoFyULBu04YW/TvOH/5nvYrNvWZs0s/di3Xb5USyVezvr3+kYtU7dRvPmvl1FUsIygaj63vgkG7kF17UxpBRIweqWFYWMhSA3eRBRhP4HBN6nE2aoEXdv924XMXe/KfO8nLB+TqDTQyYO/DO/rpdkFFnxAht5kikiM43vLdFHfNpjc7GMGq4NpT076+ziEQiyNyB7kuFkO8NZjhBYI28frahEHje0GSRvgAega9nOw7fm80EEkoxRjQc1/3k3XWbVaxnT21U69Fdz7WRTP0qdUDGkGieNmOMHK7NHf/z1E4V27r9gIp1L9V1QcO7/miDiu3df0zFdu/WZazfqA0fu2p0Rq2Rw3WGqjnfGatiGREwr4BMKsiQgujojBnWMeAmswjMnoXGALoeTSCEEEIIIcQKF4CEEEIIIT6DC0BCCCGEEJ/BBSAhhBBCiM8wm0Csolxo2gACZnS9BBDNIqE3PAoJK0EWCAfsqI8uGBLLTuLg+uhiQR2bOEFn/agcoTM51OzZr2L79tap2N7d/1Kxo0erVaxnNy1qnnvVFBXLz9cZGmIxbeQIAPEqEuyjzfnjwLQx/mwtqH/9jfdVbMcuLTiuGNBVH7f5kIp9+IEWMH+0Pfm40iJ9/1urj6jYH57cqmIhR2cPOH+8NrJ06xZVsRjILtO/olzFnPBOFWtuBeJ3kPkmntBlIBlxMKjPbWnW5/Yr00L0cWf3U7H33tMi+298vb+KFXbRpqQEqDMUTkMjmXZ0jTkrOUtOv/Le6piN72ujzaZNOrZnjxbJDxig77+4RI+97OxMFYPZiqDvwjZftiMdRxc0gdrdnejeKp6HUV0XcFxGSp2PHtVjdPnLb6vYGYN1v7h4xjkqlmoyaa9uIWD8GtRfz2Wl3XS5L6/coWIH6nS7NzU2qtj+A3rOQ8+sa74eewMH9FSxK2bp7D/lfbWpL+DosdfaqtsAmZJQF4U9Oc2sNngNAoxa6B3nwvDhJuMOHN00gRBCCCGEECtcABJCCCGE+AwuAAkhhBBCfAYXgIQQQgghPsNsAoFCQ+MO7tZlphsxo/VcvLs4MJqkGFesGmcknEd1awaGii75OgvGyKICFUsM1SLa1lZtqIiD3dWj0Rx9LqhLLKYzPuA2BoYP1FYgG0FjoxZiDxlcrmLr12vh/QvLX1exyuHDVOz55dtVrE/vAhXb/UlN0s9PfPyBOmbnp9rIMWWiNvM0N+ld91e9qo0s0y4ao2LZ2TpTAKK2VptbDh/T9esT1n2qVTc7zjQBnjfSPiNTyZTJVSq2u+ZDFdu4QT/bceNH66qA+sWB2t+6y35LSiMUFenML+d/U4v999XWqtjmTbqvrF2rDQV5edr0M3yE7rM9emhTQCgEDD6gTZDxDZHwOOuHNZMBfD3AKtuyOwSM9xuLJZsRsrP1PFhYWKpiH2zR2WAmna/ny0imNo1BAxbIRFVQoPteHsiStK26XsUaG3SbFBfre6saVa5ifXoX6nOLdB+NRPSchN4ZLa36nRECc0gIPXAwlqGh1PgOShezidXqq0KGSDQurKYsOE/bToWXS/9UQgghhBDy7wgXgIQQQgghPoMLQEIIIYQQn8EFICGEEEKIz7CbQMBaEYkjsVBXX8/NzvF2wbFRWAmFpcn1C4WNZgejmBNmywBZIJpi2twRQLuLg6V8RkSX0dSid4RHKlKgX4YCbgeqYcFxxmgoqBv10ovPV7G/v/Kuiv1zzQYV69NdC52/9f2xKpYVSRaJ7/vXQXVMl3wtzO7bV2fBqDugxdpPP7NaxZY9s1LFhlWeoWKrVn2iYpGQ7itFhVpM3tIC+o/N7yHo90N0XBz02/wu2So2Y/okFVuzZo2K1R3QWQZKuxerWKxFZxlApgAoG0/JzoOMLHEwRxUV6f408ZvjVewAMIt89JHOGrPu3fdUrKxcG3wGDapQsZxsYDxAJh2U8QBMXtbMTgirYB/FrHO3tS7o/ZDaR6NRnfHijDP6qtjfVtWo2KYP9His+tpAFUNZXnKA+aSuTmfu+HCLzvbUr0z3vTnf0WarrCw9eYfAm74FGP0SjjZ3NDVp1xh6F4SRQQGMPjfPEYHq4sZQqq6PYkbTinUNAs8FJbspA8EvgIQQQgghPoMLQEIIIYQQn8EFICGEEEKIz+ACkBBCCCHEZ5hNIAir+BJKQ8FxyBhiFYJ6KfrE1wcxsDs/Uoxi4TwSsNvq4hhMKyJYHBoKokcOROIJXT+EvdnRDuv62SYcLUyO5uo6z5jxDRVradZC/khEn+sgAXxK/bp104YKoPuVlhZd38ICvZv+ty6eoGLvrNMZJNa9s1nFmo5rkfj3v/t1FSsu0saLVlA/bPBBgmO0Y70Gab+bgdlo0GBt7hDRJpXmRn2/oCowowAyqyHU7YICUHYUx9HjArVdafcSFUMZPmpq9qrYxvc2qtjWrdpAMuxMbRjKAFkbENiggfqAtT3TN20grIYUbEJD76DkWCCg54ARlf1UbO8+beZZ9ap+PnHwKi3rqzOLNDZoc9lf/qrngTpQ7uXf0plpMrWXRVpj2rQR01OjBMBoRuZENE+jB5QA7yD87texBDIqwfVAeqYSeIzVkIQMq14bPtyYWF3AL4CEEEIIIT6DC0BCCCGEEJ/BBSAhhBBCiM/gApAQQgghxGcEHKPScPlfnrNd0LikRAJPq5DYepwbEXJqGfbd6nUDIOFqIm67f+tu4ALE0DArCxLRAqkuqgvMwAKEv9BA4tgyyeB70yFURhCYctCO7a0xfW44JYUCbHcoLkfiZZBZJaidFwFgyDlyTBsgUMKZ3Fyt/k7E9C7+IeO4wJl59HFuTFmojIwM3QatrdoYgsuw1kXHLNOeNcsNPhf1Cx3LytLZIhoaGlTs4493qFhZmc5ckZOjM00kgOEMqfPNJjSPM3fY5310rtEYktIEMZD5RcAYPXhQj8dXX9fZW7ZWazNPAKRlOVSnn0UoqPvAhHGDVGzSeTobjBMHJi9rBhYV+QKZJqCpAlwQjjObURSD+h4oIaV+MJuH8RYQOONZx2e+sRpDLr/iGtNx/AJICCGEEOIzuAAkhBBCCPEZXAASQgghhPgMLgAJIYQQQnyGOROI3VDhrUEDic6tAnM3Isq0dxcHQIG9cad7bICwZUwJAREyKhcaPmBGF5t4NQjE1LDd4bOwZZ9ABhKsN9bHIeOB7WK2w1C7J0AGiURMmx3y8iK6DNACcSD+DsOxgvoAyBATAOlBjFjHATouBtoAXw/1PZRNB51r+T0XGV68NTEg/0xrqzbuoP45ZMgQVLKtLjCRQ/rmIOscap+ndN/DpiSEcZymHAfnRnCt0pJcFbto0mgVG1V7SMUamvSz7ZKnswR166rL6JKnjSGtyMwD7iMI3A3YBILcE6hdQB8AR6HJEBrnXJiDoIHEYD6xGl7c4OXaoiPORfALICGEEEKIz+ACkBBCCCHEZ3ABSAghhBDiM7gAJIQQQgjxGa5MIEjYHgQiUq8zd7jZdd6eBeHU58VBNg+rcBxm1TCKSJFo2lqGFdgmqJkMu7B/FjSFYCyBC7GVC7C0s5sd8dHVoZkJZYgBGU6QkSMjpIeuA/uxbYwmHKvoHrWL7Vy7ftlqwLHuqG8pw2YewVlFrFmN0r8eNlTYzDJez6vWsWGdf6zzHr4345mh5HOROQERj2sjRzRXz7+DCkp13WB9dZu0xnRWkuYWXS5qTvhskUHOONtax5T5mRnNjvY+YLzeqasBcTMG3KxB3BhI3LQdvwASQgghhPgMLgAJIYQQQnwGF4CEEEIIIT6DC0BCCCGEEJ9hNoGg7AFhJER3sUu8G4GjNWNIHO2mniZWAbJVDO1ml2/UTtbd9GHboTK8FphjhbCpLug43KdQXdDlAif9ub2Yu93fXYjajc/CAVkB3JioYF2MGmQ3bWUd3+kK1rGRBV0LnWu9L9vzRs8M9W1kQvN6DrHiRgDfzmRjCsJMN4DULDmJhHH8gG8kqJliMZ2ZB41RPA+CbB7I52c03NnnZJuxKJFIvwwrrrJqGa5nTLJlupYIno+sdeuMjCFW+AWQEEIIIcRncAFICCGEEOIzuAAkhBBCCPEZXAASQgghhPiMgONG+UsIIYQQQv7t4BdAQgghhBCfwQUgIYQQQojP4AKQEEIIIcRncAFICCGEEOIzuAAkhBBCCPEZXAASQgghhPgMLgAJIYQQQnwGF4CEEEIIIT6DC0BCCCGEEJ/x/wH5NU0zw5ohdQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted text: ashmic Jr .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "inference_path = r\"C:\\Users\\ahmed\\Desktop\\STP\\prescription-data\\annotations\\Beauty_prescription_7_resized_box1.png\"\n",
    "new_image = Image.open(inference_path).convert(\"RGB\")\n",
    "pixel_values = processor(new_image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "\n",
    "\n",
    "generated_ids = model.generate(pixel_values)\n",
    "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 8))  # Adjust figure size as needed\n",
    "plt.imshow(new_image)\n",
    "plt.axis('off')  # Hide axes\n",
    "plt.title(f\"Predicted text: {generated_text}\", fontsize=12, pad=10)\n",
    "plt.show()\n",
    "\n",
    "# Print predicted text\n",
    "print(f\"Predicted text: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
